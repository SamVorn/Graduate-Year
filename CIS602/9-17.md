# Vocabulary Building Process
1. Tokenization breaks down the input text nto individual tokens
2. each unique token is added to the vocabulary in alphabetical order

## Adding special context tokens
- unknown words
    - when encountering unknown words not in training vocab, tokenizer fails
- solution: Special tokens
    - <|unk|> - replace unknown words
    - 
## Notion of Bytes in BPE (Byte Pair Encoding)
## BPE Vocab
- the goal of the BPE tokenization algorithm is to build a vocabulary of comonly occuring subwords 
- or complete words
### outline
1. identify frequent pairs
- in each iteration, scan the text to find the most commonly occuring pair of bytes
2. replace and record
- replace that pair with a new placeholder ID
- record this mapping in a lookup table
- the size of the lookup table is a